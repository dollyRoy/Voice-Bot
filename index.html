<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Voice Gemini AI Assistant</title>
  <style>
    :root {
      --bg: #f0f0f0;
      --text: #1f2937;
      --card: #ffffff;
      --primary: #6366f1;
      --danger: #ef4444;
      --muted: #6b7280;
    }
    [data-theme="dark"] {
      --bg: #1f2937;
      --text: #f9fafb;
      --card: #111827;
      --primary: #818cf8;
      --danger: #f87171;
      --muted: #d1d5db;
    }

    body {
      background-color: var(--bg);
      color: var(--text);
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
      transition: background 0.3s ease, color 0.3s ease;
    }

    .container {
      background-color: var(--card);
      border-radius: 16px;
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.2);
      padding: 2rem;
      width: 100%;
      max-width: 500px;
      text-align: center;
      transition: background 0.3s ease;
    }

    h1 {
      font-size: 2rem;
      margin-bottom: 1rem;
    }

    .btn {
      font-size: 1rem;
      padding: 0.75rem 1.5rem;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      margin: 1rem 0;
      transition: background 0.3s ease;
    }

    .record-btn {
      background-color: var(--primary);
      color: white;
    }

    .recording {
      background-color: var(--danger);
    }

    .btn:hover {
      filter: brightness(90%);
    }

    .status {
      margin-top: 1rem;
      color: var(--muted);
    }

    .output {
      margin-top: 1rem;
      font-weight: 500;
    }

    .wave {
      display: flex;
      justify-content: center;
      margin: 1rem auto;
      height: 16px;
    }

    .wave span {
      display: block;
      width: 5px;
      height: 100%;
      margin: 0 2px;
      background: var(--primary);
      animation: wave 1s infinite ease-in-out;
    }

    .wave span:nth-child(2) {
      animation-delay: 0.1s;
    }

    .wave span:nth-child(3) {
      animation-delay: 0.2s;
    }

    @keyframes wave {
      0%, 100% { transform: scaleY(0.4); }
      50% { transform: scaleY(1); }
    }

    .theme-toggle {
      background: none;
      border: 2px solid var(--primary);
      color: var(--primary);
      padding: 0.4rem 1rem;
      border-radius: 8px;
      cursor: pointer;
      font-size: 0.9rem;
      margin-top: 1rem;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body data-theme="light">
  <div class="container">
    <h1>üéôÔ∏è Voice AI Assistant</h1>
    <button id="recordBtn" class="btn record-btn">Start Recording</button>
    <div id="wave" class="wave hidden">
      <span></span><span></span><span></span>
    </div>
    <p id="status" class="status">Click to start talking</p>
    <p id="transcribedText" class="output">Transcribed Text: </p>
    <p id="geminiResponse" class="output">Gemini Response: </p>
    <button class="theme-toggle" id="themeToggle">Toggle Dark/Light</button>
    <audio id="ttsAudio" controls style="margin-top: 16px; width: 100%; display: none;"></audio>
  </div>

  <script>
    const recordBtn = document.getElementById('recordBtn');
    const status = document.getElementById('status');
    const transcribedText = document.getElementById('transcribedText');
    const geminiResponse = document.getElementById('geminiResponse');
    const wave = document.getElementById('wave');
    const themeToggle = document.getElementById('themeToggle');
    const body = document.body;
    const audioPlayer = document.getElementById('ttsAudio');

    let mediaRecorder;
    let audioChunks = [];

    themeToggle.addEventListener('click', () => {
      body.dataset.theme = body.dataset.theme === 'dark' ? 'light' : 'dark';
    });

    recordBtn.addEventListener('click', async () => {
      if (recordBtn.textContent === 'Start Recording') {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          mediaRecorder = new MediaRecorder(stream);
          audioChunks = [];

          mediaRecorder.ondataavailable = event => audioChunks.push(event.data);

          mediaRecorder.onstop = async () => {
            const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            stream.getTracks().forEach(track => track.stop());
            await sendAudioToAPI(audioBlob);
          };

          mediaRecorder.start();
          recordBtn.textContent = 'Stop Recording';
          recordBtn.classList.add('recording');
          wave.classList.remove('hidden');
          status.textContent = 'Recording...';
        } catch (err) {
          status.textContent = 'Microphone access denied.';
          console.error(err);
        }
      } else {
        mediaRecorder.stop();
        recordBtn.textContent = 'Start Recording';
        recordBtn.classList.remove('recording');
        wave.classList.add('hidden');
        status.textContent = 'Processing...';
      }
    });

    async function sendAudioToAPI(audioBlob) {
      const formData = new FormData();
      formData.append('model_id', 'scribe_v1');
      formData.append('file', audioBlob, 'recording.wav');

      try {
        const response = await fetch('https://api.elevenlabs.io/v1/speech-to-text',  {
          method: 'POST',
          headers: {
            'xi-api-key': 'sk_73277bcf2826ff573ba8fab294393646f0ac9f882750a9ca'
          },
          body: formData
        });

        const data = await response.json();
        if (data.text) {
          transcribedText.textContent = `Transcribed Text: ${data.text}`;
          status.textContent = 'Sending to Gemini AI...';
          await sendToGeminiAPI(data.text);
        } else {
          status.textContent = 'Transcription failed.';
        }
      } catch (err) {
        console.error(err);
        status.textContent = 'Error transcribing audio.';
      }
    }

    async function sendToGeminiAPI(text) {
      const requestBody = {
        contents: [{ parts: [{ text }] }]
      };

      try {
        const response = await fetch(
          "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyCLMeRAkFg8H2wQajyIMbbtqQdscm5Qk2o",

          {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(requestBody)
          }
        );

        const data = await response.json();
        const reply = data?.candidates?.[0]?.content?.parts?.[0]?.text || 'No response';
        geminiResponse.textContent = `Gemini Response: ${reply}`;
        status.textContent = 'Playing response...';
        playTextToSpeech(reply);
      } catch (err) {
        console.error(err);
        status.textContent = 'Gemini API failed.';
      }
    }

    async function playTextToSpeech(text) {
      const ttsPayload = {
        text,
        model_id: "eleven_multilingual_v2"
      };

      try {
        const response = await fetch(
          'https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128',
          {
            method: 'POST',
            headers: {
              'xi-api-key': 'sk_354a70b05f07e39e9b0726e2d9092f4ce562ddcfa3787876',
              'Content-Type': 'application/json'
            },
            body: JSON.stringify(ttsPayload)
          }
        );

        if (!response.ok) {
          throw new Error("TTS API failed");
        }

        const audioBlob = await response.blob();
        const audioUrl = URL.createObjectURL(audioBlob);
        audioPlayer.src = audioUrl;
        audioPlayer.style.display = 'block';
        audioPlayer.play();
        status.textContent = 'Audio playback started';
      } catch (err) {
        console.error(err);
        status.textContent = 'Error generating or playing audio';
      }
    }
  </script>
  <script type="module">
    // Import the functions you need from the SDKs you need
    import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-app.js";
    import { getAnalytics } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-analytics.js";
    // TODO: Add SDKs for Firebase products that you want to use
    // https://firebase.google.com/docs/web/setup#available-libraries
  
    // Your web app's Firebase configuration
    // For Firebase JS SDK v7.20.0 and later, measurementId is optional
    const firebaseConfig = {
      apiKey: "AIzaSyCEX92u6X_Rh4IbmYHnrPg88f0drZyrb4E",
      authDomain: "voicebot-63c2d.firebaseapp.com",
      projectId: "voicebot-63c2d",
      storageBucket: "voicebot-63c2d.firebasestorage.app",
      messagingSenderId: "786377612757",
      appId: "1:786377612757:web:05f000eac07e79589e715c",
      measurementId: "G-SHVYE540D2"
    };
  
    // Initialize Firebase
    const app = initializeApp(firebaseConfig);
    const analytics = getAnalytics(app);
  </script>
</body>
</html>
